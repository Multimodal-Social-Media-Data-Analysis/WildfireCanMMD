{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19731,
     "status": "ok",
     "timestamp": 1730323333929,
     "user": {
      "displayName": "multimedia research",
      "userId": "00317422700387342345"
     },
     "user_tz": 240
    },
    "id": "YytNEg9nFC9e",
    "outputId": "d1f21c79-1467-4d49-9057-45fcd1d8b6c8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVaXt2DCFKsO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Palette\")        #ignore 'palette images expressed in bytes' warning\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"os.fork()\")   #ignore os.fork() multithreading warning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Glyph\")          #ignore warning about emojies missing from font for displaying predictions\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "from transformers import RobertaTokenizer, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNWGOYxGs-nA"
   },
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, text_encodings, image_paths, labels, image_processor):\n",
    "        self.text_encodings = text_encodings\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.text_encodings.items()}\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        image = self.image_processor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "        item['image'] = image\n",
    "        item['label'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.text_model = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.image_model = ViTModel.from_pretrained(IMAGE_MODEL)\n",
    "        self.fc = nn.Linear(self.text_model.config.hidden_size + self.image_model.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        # text\n",
    "        text_outputs = self.text_model(input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.last_hidden_state[:, 0, :]  # Take the [CLS] token\n",
    "        image_features = self.image_model(pixel_values=images).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # concatenate\n",
    "        combined_features = torch.cat((text_features, image_features), dim=1)\n",
    "        logits = self.fc(combined_features)\n",
    "        return logits\n",
    "\n",
    "def train(model, dataloader, optimizer, device, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].squeeze().to(device)\n",
    "        attention_mask = batch['attention_mask'].squeeze().to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "def evaluate(model, dataloader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch['input_ids'].squeeze().to(device)\n",
    "            attention_mask = batch['attention_mask'].squeeze().to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    report = classification_report(all_targets, all_predictions, target_names=label_encoder.classes_, digits=4)\n",
    "    print(report)\n",
    "    return all_targets, all_predictions, avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMC6oWBgwZaE"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 8\n",
    "IMAGE_MODEL = 'google/vit-base-patch16-384'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhQ6X_1ZwW0H"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset_path = '/content/drive/My Drive/multimodal_classifier/data/WildFireCan-MMD.csv'\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# drop uneeded columns\n",
    "columns_to_drop2 = ['tweet_id', 'img_id', 'posted_at', 'author_id', 'author_loc', 'author_name', 'author_usrname', 'media_keys', 'urls', 'predicted_label', 'contains_personal_info']\n",
    "dataset = dataset.drop(columns=columns_to_drop2)\n",
    "\n",
    "# fix image paths\n",
    "dataset['image'] = dataset['image'].apply(lambda x: x.split('\\\\')[7])\n",
    "base_path2 = '/content/drive/My Drive/multimodal_classifier/data/images/'\n",
    "dataset['image'] = dataset['image'].apply(lambda x: base_path2 + x)\n",
    "\n",
    "# shuffle data\n",
    "dataset = dataset.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "# Split the data into train and test sets (80/20 split), stratifying by 'label'\n",
    "train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=RANDOM_STATE, stratify=dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1729659421905,
     "user": {
      "displayName": "multimedia research",
      "userId": "00317422700387342345"
     },
     "user_tz": 240
    },
    "id": "6tiBwB2vux2k",
    "outputId": "45a95f7b-5ddb-4f05-9ad1-611b4b8c746e"
   },
   "outputs": [],
   "source": [
    "class_counts = train_df['label'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3000,
     "status": "ok",
     "timestamp": 1729659429474,
     "user": {
      "displayName": "multimedia research",
      "userId": "00317422700387342345"
     },
     "user_tz": 240
    },
    "id": "DyTNZKLauWTJ",
    "outputId": "cfac059d-ab06-4508-9048-b470cd4404e2"
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "image_processor = AutoImageProcessor.from_pretrained(IMAGE_MODEL)\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_data(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=200, return_tensors='pt')\n",
    "\n",
    "train_encodings = tokenize_data(train_df['text'].tolist())\n",
    "test_encodings = tokenize_data(test_df['text'].tolist())\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_df['label'].values)\n",
    "test_labels = label_encoder.transform(test_df['label'].values)\n",
    "\n",
    "train_dataset = MultimodalDataset(train_encodings, train_df['image'].tolist(), train_labels, image_processor=image_processor)\n",
    "test_dataset = MultimodalDataset(test_encodings, test_df['image'].tolist(), test_labels, image_processor=image_processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=12, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=12, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGJBLa1xwTWx"
   },
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1243,
     "status": "ok",
     "timestamp": 1729659439424,
     "user": {
      "displayName": "multimedia research",
      "userId": "00317422700387342345"
     },
     "user_tz": 240
    },
    "id": "imz7WoE5FRdG",
    "outputId": "68dc827c-4c04-483a-92d7-575cf0eb6bea"
   },
   "outputs": [],
   "source": [
    "num_labels = len(label_encoder.classes_)\n",
    "model = MultimodalModel(num_labels)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfH7m_phbZqg"
   },
   "outputs": [],
   "source": [
    "# Directory to save the best model\n",
    "model_save_path = '/content/drive/My Drive/multimodal_classifier/model/early_fusion/model_2-head'\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "best_test_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9qo0bEwFUwG"
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, device, criterion)\n",
    "    _, _, test_loss, test_accuracy = evaluate(model, test_loader, device, criterion)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    print(f'Training Acc:  {train_accuracy:.4f}')\n",
    "    print(f'Training Loss: {train_loss:.4f}')\n",
    "\n",
    "    # Save the best model based on validation accuracy\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_path, 'model.bin'))\n",
    "        tokenizer.save_pretrained(model_save_path)\n",
    "        print(f\"Best model saved with test accuracy: {best_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1303,
     "status": "ok",
     "timestamp": 1729659444508,
     "user": {
      "displayName": "multimedia research",
      "userId": "00317422700387342345"
     },
     "user_tz": 240
    },
    "id": "QlTk78KkPD3F",
    "outputId": "a42a3b9b-234b-463f-c915-c7fe332f9e72"
   },
   "outputs": [],
   "source": [
    "# load saved model and evaluate\n",
    "model_save_path = '/content/drive/My Drive/multimodal_classifier/model/early_fusion/model_2-head'\n",
    "model.load_state_dict(torch.load(os.path.join(model_save_path, 'model.bin')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38307,
     "status": "ok",
     "timestamp": 1729659487644,
     "user": {
      "displayName": "multimedia research",
      "userId": "00317422700387342345"
     },
     "user_tz": 240
    },
    "id": "fAY2Nx7T-fiX",
    "outputId": "82dfe701-9d99-41c0-f790-3f3f206ba32e"
   },
   "outputs": [],
   "source": [
    "true_labels, pred_labels, avg_loss, test_accuracy = evaluate(model, test_loader, device, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "executionInfo": {
     "elapsed": 1679,
     "status": "ok",
     "timestamp": 1722382248881,
     "user": {
      "displayName": "multimedia research",
      "userId": "00317422700387342345"
     },
     "user_tz": 240
    },
    "id": "4jHWfdYXFXcf",
    "outputId": "d8c43415-7f79-447c-a6fe-76f8a54dec6c"
   },
   "outputs": [],
   "source": [
    "# show confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "disp.plot(xticks_rotation=90)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDiJVhjWUHnmRe5UvMgUd9",
   "collapsed_sections": [
    "SlBm2TlhOeg0",
    "oMC6oWBgwZaE",
    "dGJBLa1xwTWx",
    "4leLlWZdE9fu",
    "ebCC3vT9oEpu"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
