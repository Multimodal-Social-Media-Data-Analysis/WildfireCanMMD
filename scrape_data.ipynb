{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  # No truncation of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEARER_TOKEN = \"\"\n",
    "CLIENT = tweepy.Client(BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_ab_query = '(#BCwildfire OR #BCfire OR #ABWildfire OR #albertawildfire OR #ABFire) -has:videos has:images lang:en -is:retweet -is:quote -is:reply'\n",
    "#bc_ab_jasper_query = '(#JasperStrong OR #JasperWildfire OR #JasperAB OR #BCwildfire OR #BCfire OR #ABWildfire OR #albertawildfire OR #ABFire) -has:videos has:images lang:en -is:retweet -is:quote -is:reply'\n",
    "\n",
    "wildfire_start = '2022-05-01T00:00:00Z' #may 1st\n",
    "wildfire_end = '2022-10-01T00:00:00Z'   #oct 1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic-tier $100/mo\n",
    "def scrapeTweets(query, next_token=None):\n",
    "    response = CLIENT.search_recent_tweets(\n",
    "        query = query,\n",
    "        max_results = 100,\n",
    "        media_fields = ['media_key', 'type', 'preview_image_url','url','public_metrics','duration_ms', 'width'],\n",
    "        place_fields = ['country', 'country_code', 'full_name', 'geo', 'id', 'name'],\n",
    "        tweet_fields = ['created_at', 'geo', 'public_metrics', 'text','id', 'entities', 'lang', 'attachments'],\n",
    "        user_fields =  ['username', 'name', 'public_metrics', 'description', 'location','created_at','entities','id', \n",
    "                       'pinned_tweet_id','profile_image_url','protected','url','verified','withheld'],\n",
    "        next_token = next_token,\n",
    "        expansions = ['attachments.media_keys', 'author_id', 'geo.place_id'])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pro-tier $5000/mo\n",
    "def scrapeTweets(query, start_time, end_time, next_token=None):\n",
    "    response = CLIENT.search_all_tweets(\n",
    "        query = query,\n",
    "        max_results = 500,\n",
    "        start_time = start_time,\n",
    "        end_time = end_time,\n",
    "        media_fields = ['media_key', 'type', 'preview_image_url','url','public_metrics','duration_ms', 'width'],\n",
    "        place_fields = ['country', 'country_code', 'full_name', 'geo', 'id', 'name'],\n",
    "        tweet_fields = ['created_at', 'geo', 'public_metrics', 'text','id', 'entities', 'lang', 'attachments'],\n",
    "        user_fields =  ['username', 'name', 'public_metrics', 'description', 'location','created_at','entities','id', \n",
    "                       'pinned_tweet_id','profile_image_url','protected','url','verified','withheld'],\n",
    "        next_token = next_token,\n",
    "        expansions = ['attachments.media_keys', 'author_id', 'geo.place_id'])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get x pages of responses\n",
    "responses = []\n",
    "nxt_tkn = None\n",
    "for i in range(200):\n",
    "    response = scrapeTweets(bc_ab_query, wildfire_start, wildfire_end, nxt_tkn)\n",
    "    response = response._asdict()\n",
    "    responses.append(response)\n",
    "    try:\n",
    "        nxt_tkn = response['meta']['next_token']\n",
    "    except:\n",
    "        break\n",
    "    print('Collected', len(responses), 'pages of tweets...')\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Collected', len(responses), 'pages of tweets...')\n",
    "for i in range(len(responses)):\n",
    "    data_length = len(responses[i]['data']) if responses[i].get('data') else 0\n",
    "    print('Page', i, 'length:', data_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tweets = 0\n",
    "num_photos = 0\n",
    "num_users = 0\n",
    "\n",
    "for i in range(len(responses)):\n",
    "    data_length = len(responses[i]['data']) if responses[i].get('data') else 0\n",
    "    num_tweets += data_length\n",
    "    num_photos += len(responses[i]['includes']['media'])\n",
    "    num_users += len(responses[i]['includes']['users'])\n",
    "\n",
    "print('Total num. tweets:', num_tweets)\n",
    "print('Total num. media:', num_photos)\n",
    "print('Total num. users:', num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataframe from responses - for scraping with images\n",
    "new_df = pd.DataFrame(columns=['tweet_id', 'img_id', 'posted_at', 'author_id', 'author_loc', 'author_name', 'author_usrname', 'text', 'media_keys', 'urls', 'path']) # place,\n",
    "\n",
    "for i in range(len(responses)):\n",
    "    media_dict = {media['media_key']: media['url'] for media in responses[i]['includes']['media']}\n",
    "    for j in range(len(responses[i]['data'])):\n",
    "        #print('i:', i, '|', 'j:', j)\n",
    "        tweet_id = responses[i]['data'][j]['id']\n",
    "        created_at = responses[i]['data'][j]['created_at']\n",
    "        author_id = responses[i]['data'][j]['author_id']\n",
    "        #place_id = responses[i]['data'][j]['geo']['place_id']\n",
    "        text = responses[i]['data'][j]['text']\n",
    "        media_keys = [d['media_key'] for d in responses[i]['data'][j]['entities']['urls'] if 'media_key' in d]\n",
    "\n",
    "        # Retrieve URLs corresponding to the media keys\n",
    "        media_urls = [media_dict.get(key, None) for key in media_keys]\n",
    "\n",
    "        # Find the matching user\n",
    "        user = next((u for u in responses[i]['includes']['users'] if u['id'] == author_id), None)\n",
    "        if user:\n",
    "            author_loc = user.get('location', 'n/a')\n",
    "            author_name = user.get('name', 'n/a')\n",
    "            author_usrname = user.get('username', 'n/a')\n",
    "        else:\n",
    "            author_loc = 'n/a'\n",
    "            author_name = 'n/a'\n",
    "            author_usrname = 'n/a'\n",
    "        \n",
    "        # # Find the matching place\n",
    "        # place = next((u for u in responses[i]['includes']['places'] if u['id'] == place_id), None)\n",
    "        # if place:\n",
    "        #     place = place.get('full_name', 'n/a')\n",
    "        # else:\n",
    "        #     place = 'n/a'\n",
    "\n",
    "        # Create img_ids and paths\n",
    "        img_ids = []\n",
    "        paths = []\n",
    "        for url in media_urls:\n",
    "            if url is None:\n",
    "                img_ids.append(None)\n",
    "                paths.append(None)\n",
    "            else:\n",
    "                img_id = url.split('/')[4].split('.')[0]\n",
    "                ext = url.split('/')[4].split('.')[1]\n",
    "                path = f\"{tweet_id}_{img_id}.{ext}\"\n",
    "                img_ids.append(img_id)\n",
    "                paths.append(path)\n",
    "        \n",
    "        # Append to df\n",
    "        for k in range(len(media_urls)):\n",
    "            new_row = {\n",
    "                \"tweet_id\": tweet_id,\n",
    "                \"img_id\": img_ids[k],\n",
    "                \"posted_at\": created_at,\n",
    "                \"author_id\": author_id,\n",
    "                #\"place\": place,\n",
    "                \"author_loc\": author_loc,\n",
    "                \"author_name\": author_name,\n",
    "                \"author_usrname\": author_usrname,\n",
    "                \"text\": text,\n",
    "                \"media_keys\": media_keys[k] if k < len(media_keys) else None,\n",
    "                \"urls\": media_urls[k],\n",
    "                \"path\": paths[k]\n",
    "            }\n",
    "            new_df.loc[len(new_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with videos instead of images\n",
    "new_df.dropna(subset=['urls'], inplace=True)\n",
    "new_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some samples\n",
    "sampled_df = new_df.sample(n=9)\n",
    "# Create a plot\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "for i, (index, row) in enumerate(sampled_df.iterrows()):\n",
    "    url = row['urls']\n",
    "    text = row['text']\n",
    "    # Wrap the text to fit the plot\n",
    "    wrapped_text = \"\\n\".join(textwrap.wrap(text, width=40))\n",
    "    # Fetch the image from the URL\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    # Plot the image with the wrapped text as the title\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(wrapped_text, fontsize=12)\n",
    "    axes[i].axis('off')\n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images from dataset to a local folder\n",
    "def download_imgs(df, folder_pth):\n",
    "    failed_ctr = 0\n",
    "    failed_list = []\n",
    "    log_messages = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Downloading images\"):\n",
    "        url = row['urls']\n",
    "        path = row['path']\n",
    "        image_path = os.path.join(folder_pth, path)\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            log_messages.append(f\"File already exists, skipping: {i}/{len(df)-1} {path}\")\n",
    "            continue\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(image_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            log_messages.append(f\"Downloaded and saved: {i}/{len(df)-1} {path}\")\n",
    "        else:\n",
    "            log_messages.append(f\"Failed to download: {i}/{len(df)-1} {url}\")\n",
    "            failed_ctr += 1\n",
    "            failed_list.append((i, path))\n",
    "\n",
    "    if failed_ctr > 0:\n",
    "        log_messages.append(f'The following number of images failed to download: {failed_ctr}')\n",
    "        log_messages.append(f\"{failed_list}\")\n",
    "\n",
    "    # Write all log messages to a file after the loop\n",
    "    with open('log.txt', 'a') as log_file:\n",
    "        for message in log_messages:\n",
    "            log_file.write(message + '\\n')\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "download_imgs(df, 'dataset_image_folder')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
